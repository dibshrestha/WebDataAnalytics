{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API and Basic Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import urllib.request, urllib.error, urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckey = ''\n",
    "csecret = ''\n",
    "atoken = ''\n",
    "asecret = ''\n",
    "\n",
    "# the authentication process for tweepy\n",
    "auth = tweepy.OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will want to keep track of the text, tweet ID, and tweet creation time, so we will create three lists to hold this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see how we can use the twitter API.  Let's say we want to use the search tool, from the list above, lets first click on the link in the developer page and see what our options are.  Notice point 2 from below.  The API is simply a URL that you go to and in the URL you specify what you want.  tweepy simplifies this and allows us to loop through and be more efficient with our time.  For example, \"https://twitter.com/search?q=%40twitterapi\"  is the url in the example.  The first term after twitter.com ('search') is the method, and each question mark after that introduces a parameter.  Here there is only one parameter, q, which represents the term you want to search for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('kickstarter.csv')\n",
    "q_list=df['name'].values.T.tolist()\n",
    "project=df['project_id'].values.T.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go through the elements above, and see which ones may be useful to you.  For us, we will use the text and created_at element, and we will also use the ID element, but we only use this because of a potential problem called pagination which we need to deal with.  To understand pagination, you can refer to the following website, or just use the id as we have used it in our codeto bypass the issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://developer.twitter.com/en/docs/basics/cursoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis and Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tweet_text = []\n",
    "tweet_id = []\n",
    "tweet_created = []\n",
    "tweets_list=[]\n",
    "new_list=[]\n",
    "ID_list=[]\n",
    "followers_count = []\n",
    "retweet_count = []\n",
    "\n",
    "\n",
    "a=0\n",
    "b=30\n",
    "num=0\n",
    "new=-1\n",
    "while num<len(q_list):\n",
    "    break_num=0\n",
    "    rand=q_list[a:b]\n",
    "    for i in rand:\n",
    "        #if i == \n",
    "        num+=1\n",
    "        new+=1\n",
    "        break_num+=1\n",
    "        a+=1\n",
    "\n",
    "        name=i+' '+'kickstarter'+' '+'-filter:retweets'    \n",
    "        tweets = api.search(q = name , count = 50, lang='en', tweet_mode='extended')\n",
    "        if len(tweets)==0:\n",
    "\n",
    "            new_list.append(i)\n",
    "            ID_list.append(project[new])\n",
    "            tweet_text.append(' ')\n",
    "            tweet_id.append(' ')\n",
    "            tweet_created.append(' ')\n",
    "            followers_count.append(' ')\n",
    "        else:\n",
    "            for tweet in tweets:    \n",
    "                json_str = json.dumps(tweet._json)\n",
    "                project_json = json.loads(json_str)\n",
    "\n",
    "                new_list.append(i)\n",
    "                \n",
    "                ID_list.append(project[new])\n",
    "                tweet_text.append(project_json['full_text'])\n",
    "                tweet_id.append(project_json['id'])\n",
    "                tweet_created.append(str(project_json[\"created_at\"])) \n",
    "                followers_count.append(project_json['user']['followers_count'])\n",
    "                retweet_count.append(project_json['retweet_count'])\n",
    "                \n",
    "        if break_num==30:\n",
    "            #print(a,b)\n",
    "            b+=30\n",
    "            time.sleep(315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So lets clean the tweets to get only the texts for our sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdf=pd.DataFrame(list(zip(ID_list,new_list, tweet_text,followers_count,tweet_id,tweet_created,retweet_count )),columns=['Project_ID','Product Name','Tweet Text','Followers','Tweet Id','Tweet Creation Date', 'Retweet'])\n",
    "\n",
    "outputdf.to_csv('tweets_report.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sat Nov 30 00:48:54 +0000 2019',\n",
       " 'Sat Nov 30 00:45:33 +0000 2019',\n",
       " 'Thu Nov 28 15:04:34 +0000 2019',\n",
       " 'Fri Nov 22 18:33:47 +0000 2019',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Fri Nov 29 20:07:13 +0000 2019',\n",
       " 'Wed Nov 27 20:02:50 +0000 2019',\n",
       " 'Wed Nov 27 15:03:10 +0000 2019',\n",
       " 'Wed Nov 27 03:38:35 +0000 2019',\n",
       " 'Tue Nov 26 17:42:57 +0000 2019',\n",
       " 'Tue Nov 26 15:39:48 +0000 2019',\n",
       " 'Tue Nov 26 11:58:03 +0000 2019',\n",
       " 'Tue Nov 26 05:07:01 +0000 2019',\n",
       " 'Sun Nov 24 18:44:26 +0000 2019',\n",
       " 'Sat Nov 23 04:24:04 +0000 2019',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Thu Nov 28 21:47:38 +0000 2019',\n",
       " 'Wed Nov 27 22:18:16 +0000 2019',\n",
       " 'Thu Nov 21 02:34:06 +0000 2019',\n",
       " 'Wed Nov 27 10:59:26 +0000 2019',\n",
       " 'Mon Nov 25 22:48:32 +0000 2019',\n",
       " 'Sun Nov 24 06:47:29 +0000 2019',\n",
       " 'Sat Nov 23 21:54:41 +0000 2019',\n",
       " 'Sat Nov 30 06:28:04 +0000 2019',\n",
       " 'Sat Nov 30 06:15:46 +0000 2019',\n",
       " 'Sat Nov 30 03:19:17 +0000 2019',\n",
       " 'Thu Nov 28 21:06:46 +0000 2019',\n",
       " 'Thu Nov 28 20:23:03 +0000 2019',\n",
       " 'Wed Nov 27 02:33:47 +0000 2019',\n",
       " 'Wed Nov 27 00:44:23 +0000 2019',\n",
       " ' ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
